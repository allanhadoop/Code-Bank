import requests
from urllib import response
from bs4 import BeautifulSoup
import operator
#Beautifulsoup is more of a filtering criteria for web content
def wordcount(url):
    mylist = []
    getdata = requests.get(url).text

    souptank = BeautifulSoup(getdata,"html.parser")
    for line in souptank.findAll('p'):
        content = str(line)

        word = content.split()
        for eachword in word:
            if eachword.__contains__('href'):
                continue
            elif eachword.__contains__('</p>'):
                continue
            elif eachword.__contains__('<p>'):
                continue
            else:
                print(eachword)
                mylist.append(eachword)
    clean_list(mylist)

def clean_list(mylist):
    final_list = []
    for word in mylist:
        symbols = "~!@#$,.%^&*({}[])_+-=:\"</>?|}{\'"
        length = len(symbols)
        for counter in range(0,length):
            try:
                final_word = word.replace(symbols(counter),"")
                if len(final_word) >= 0:
                    print(final_word)
                    final_list.append(final_word)
            except TypeError:
                pass
wordcount('http://www.policydriven.com/about-us/')
